{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.transforms import Compose, Activations, AsDiscreted, Invertd, KeepLargestConnectedComponentd, Lambdad, MapTransform\n",
    "import monai.transforms as T\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from picai_eval import evaluate\n",
    "from scipy.ndimage import label\n",
    "from lightning.pytorch import seed_everything\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessPredictions(T.MapTransform):\n",
    "    \"\"\"Ensure predictions are non-overlapping 3D connected components with a single confidence score per lesion.\"\"\"\n",
    "    def __init__(self, keys, threshold=0.1):\n",
    "        super().__init__(keys)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for key in self.keys:\n",
    "            pred = data[key]\n",
    "            processed_pred = self.process_predictions(pred)\n",
    "            data[key] = processed_pred  # Replace with processed prediction\n",
    "        return data\n",
    "\n",
    "    def process_predictions(self, pred):\n",
    "        \"\"\"Processes prediction to match evaluation format.\"\"\"\n",
    "        # Binarize the prediction based on the threshold\n",
    "        binary_pred = (pred > self.threshold).astype(np.uint8)\n",
    "        \n",
    "        if binary_pred.ndim > 3:\n",
    "            binary_pred = np.squeeze(binary_pred)\n",
    "            pred = np.squeeze(pred)\n",
    "\n",
    "        # Label connected components using 26-connectivity\n",
    "        labeled_pred, num_components = label(binary_pred, structure=np.ones((3, 3, 3)))\n",
    "\n",
    "        # Create a new array to store the processed prediction\n",
    "        processed_pred = np.zeros_like(pred, dtype=np.float32)\n",
    "\n",
    "        # Iterate over each connected component (lesion)\n",
    "        for i in range(1, num_components + 1):\n",
    "            lesion_mask = (labeled_pred == i)  # Mask for the current lesion\n",
    "            \n",
    "            # Extract the prediction values of the lesion region\n",
    "            lesion_values = pred[lesion_mask]\n",
    "            \n",
    "            # Calculate the median value for the lesion region\n",
    "            lesion_median = np.median(lesion_values)\n",
    "            \n",
    "            # Assign the median value to all voxels in the lesion region\n",
    "            processed_pred[lesion_mask] = lesion_median\n",
    "       \n",
    "        processed_pred = np.expand_dims(processed_pred, axis=0)  # Add an extra dimension for consistency (as a 3D volume)\n",
    "        return processed_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Predictions:   0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Predictions: 100%|██████████| 150/150 [01:20<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 ground truth masks and 150 processed predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth_dir = \"./data/PICCAIv2/labels/val\"\n",
    "predictions_dir = \"./data/PICCAIv2/predictions/val\"\n",
    "\n",
    "run_id = '8d22yg6m'\n",
    "checkpoint = 'model-epoch=224-val_dice=0.52'\n",
    "\n",
    "# Get sorted case IDs\n",
    "case_ids = sorted(os.listdir(ground_truth_dir))\n",
    "\n",
    "ground_truths = []\n",
    "processed_predictions = []\n",
    "\n",
    "processor = ProcessPredictions(keys=[\"pred\"])\n",
    "\n",
    "for case_id in tqdm(case_ids, desc=\"Processing Predictions\"):\n",
    "    \n",
    "    case_label_dir = os.path.join(ground_truth_dir, case_id)\n",
    "    case_pred_dir = os.path.join(predictions_dir, case_id)\n",
    "\n",
    "    # Find the correct file in each case directory\n",
    "    gt_files = sorted([f for f in os.listdir(case_label_dir) if f.endswith(\".nii.gz\")])\n",
    "    pred_files = sorted([\n",
    "        f for f in os.listdir(case_pred_dir) \n",
    "        if f.endswith(\".nii.gz\") and f.startswith(f\"{run_id}_{checkpoint}\")\n",
    "    ])\n",
    "\n",
    "    # Ensure each case has exactly one label and one prediction file\n",
    "    if len(gt_files) != 1 or len(pred_files) != 1:\n",
    "        print(f\"Skipping case {case_id} due to missing or multiple files.\")\n",
    "        continue\n",
    "\n",
    "    gt_path = os.path.join(case_label_dir, gt_files[0])\n",
    "    pred_path = os.path.join(case_pred_dir, pred_files[0])\n",
    "\n",
    "    # Load ground truth\n",
    "    gt_nifti = nib.load(gt_path)\n",
    "    ground_truth = gt_nifti.get_fdata().astype(np.uint8)  # Ensure binary format\n",
    "    ground_truths.append(ground_truth.squeeze())\n",
    "\n",
    "    # Load prediction\n",
    "    pred_nifti = nib.load(pred_path)\n",
    "    pred = pred_nifti.get_fdata().astype(np.float32)  # Ensure float format\n",
    "\n",
    "    # Apply processing\n",
    "    processed_pred = processor({\"pred\": pred})[\"pred\"]\n",
    "    processed_predictions.append(processed_pred.squeeze())\n",
    "\n",
    "print(f\"Loaded {len(ground_truths)} ground truth masks and {len(processed_predictions)} processed predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(t2w_img, ground_truth, prediction, title=\"Segmentation Visualization\"):\n",
    "    \"\"\"\n",
    "    Function to visualize ground truth and predicted segmentation overlays on the T2W image.\n",
    "    \n",
    "    Parameters:\n",
    "    - t2w_img (numpy array): The grayscale T2-weighted image.\n",
    "    - ground_truth (numpy array): Ground truth segmentation mask.\n",
    "    - prediction (numpy array): Predicted segmentation mask.\n",
    "    - title (str): Title for the visualization.\n",
    "    \"\"\"\n",
    "    # Find the slice with the most segmentation in the ground truth\n",
    "    label_slices = np.sum(ground_truth, axis=(0, 1))  # Sum over H, W\n",
    "    slice_idx = np.argmax(label_slices)  # Slice index with most segmentation\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Reduce figure height\n",
    "    fig.suptitle(title, fontsize=16)  # Adjust title font size\n",
    "\n",
    "    print(ground_truth.shape)\n",
    "    print(prediction.shape)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    if t2w_img is not None:\n",
    "        axes[0].imshow(t2w_img[:, :, slice_idx], cmap=\"gray\")\n",
    "        axes[0].imshow(ground_truth[:, :, slice_idx], cmap=\"Reds\", alpha=0.8)\n",
    "        axes[0].axis(\"off\")\n",
    "    else:\n",
    "        axes[0].imshow(ground_truth[:, :, slice_idx], cmap=\"Reds\")\n",
    "        axes[0].axis(\"off\")\n",
    "        \n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    if t2w_img is not None:\n",
    "        axes[1].imshow(t2w_img[:, :, slice_idx], cmap=\"gray\")\n",
    "        confidence_map = axes[1].imshow(prediction[:, :, slice_idx], cmap=\"coolwarm\", vmin=0, vmax=1, alpha = 0.6)\n",
    "        axes[1].axis(\"off\")\n",
    "    else:\n",
    "        confidence_map = axes[1].imshow(prediction[:, :, slice_idx], cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "        axes[1].axis(\"off\")\n",
    "    \n",
    "    fig.colorbar(confidence_map, ax=axes[1], fraction=0.046, pad=0.04, label=\"Confidence (0-1)\")\n",
    "\n",
    "    plt.subplots_adjust(top=0.85)  # Adjust title placement\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics(auroc=74.13%, AP=35.07%, 150 cases, 42 lesions)\n"
     ]
    }
   ],
   "source": [
    "piccai_score = evaluate(\n",
    "    y_det = processed_predictions,\n",
    "    y_true = ground_truths\n",
    ")\n",
    "print(piccai_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
