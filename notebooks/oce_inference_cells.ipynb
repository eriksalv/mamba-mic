{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_modules.ocelot import OcelotDataModule\n",
    "from models.dynunet import DynUNetModel\n",
    "import monai.transforms as T\n",
    "from lightning.pytorch import seed_everything\n",
    "import torch\n",
    "from system import System\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.metrics import (\n",
    "    DiceMetric,\n",
    "    HausdorffDistanceMetric,\n",
    ")\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ocelot_util import (\n",
    "    normalize_crop_coords_batch,\n",
    "    evaluate_cell_detection_batch, \n",
    "    cell_detection_postprocessing_batch, \n",
    "    calculate_metrics, \n",
    "    load_ground_truth,\n",
    ")\n",
    "import random\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.\n"
     ]
    }
   ],
   "source": [
    "path = './lightning_logs/ib1kxueb/checkpoints/model-epoch=143-val_loss=0.74.ckpt'\n",
    "name = 'SwinUNETR'\n",
    "cell_classifier = System.load_from_checkpoint(checkpoint_path=path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 400 | Val: 137 | Test: 126\n"
     ]
    }
   ],
   "source": [
    "data_module = OcelotDataModule(batch_size=1, num_workers=1)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"ocelot_cell/best\", exist_ok=True)\n",
    "os.makedirs(\"ocelot_cell/worst\", exist_ok=True)\n",
    "os.makedirs(\"ocelot_cell/average\", exist_ok=True)\n",
    "os.makedirs(\"ocelot_cell/random\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_post_transform = T.Activations(softmax = True,  dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import peak_local_max\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def evaluate_cell_detection_with_coords(pred_cells, pred_classes, gt_cells, gt_classes, max_distance=15):\n",
    "    matched_gt_cells = []\n",
    "    matched_gt_classes = []\n",
    "\n",
    "    unmatched_gt_cells = []\n",
    "    unmatched_gt_classes = []\n",
    "\n",
    "    unmatched_pred_cells = []\n",
    "    unmatched_pred_classes = []\n",
    "    \n",
    "    pred_cells = pred_cells[0]\n",
    "    pred_classes = pred_classes[0]\n",
    "    gt_cells = gt_cells[0]\n",
    "    gt_classes = gt_classes[0]\n",
    "\n",
    "    for class_val in [1, 2]:\n",
    "        class_gt_cells = gt_cells[gt_classes == class_val]\n",
    "        class_pred_cells = pred_cells[pred_classes == class_val]\n",
    "\n",
    "        gt_tree = cKDTree(class_gt_cells)\n",
    "        matched_gt_indices = set()\n",
    "\n",
    "        for pred_idx, pred in enumerate(class_pred_cells):\n",
    "            neighbors = gt_tree.query_ball_point(pred, max_distance)\n",
    "            available = [i for i in neighbors if i not in matched_gt_indices]\n",
    "\n",
    "            if available:\n",
    "                closest_idx = min(available, key=lambda i: np.linalg.norm(pred - class_gt_cells[i]))\n",
    "                matched_gt_indices.add(closest_idx)\n",
    "                matched_gt_cells.append(class_gt_cells[closest_idx])\n",
    "                matched_gt_classes.append(class_val)\n",
    "            else:\n",
    "                unmatched_pred_cells.append(pred)\n",
    "                unmatched_pred_classes.append(class_val)\n",
    "\n",
    "        # Unmatched ground truths\n",
    "        for i, gt_cell in enumerate(class_gt_cells):\n",
    "            if i not in matched_gt_indices:\n",
    "                unmatched_gt_cells.append(gt_cell)\n",
    "                unmatched_gt_classes.append(class_val)\n",
    "\n",
    "    # Convert to numpy arrays for plotting\n",
    "    matched_gt_cells = np.array(matched_gt_cells)\n",
    "    matched_gt_classes = np.array(matched_gt_classes)\n",
    "\n",
    "    unmatched_gt_cells = np.array(unmatched_gt_cells)\n",
    "    unmatched_gt_classes = np.array(unmatched_gt_classes)\n",
    "\n",
    "    unmatched_pred_cells = np.array(unmatched_pred_cells)\n",
    "    unmatched_pred_classes = np.array(unmatched_pred_classes)\n",
    "\n",
    "    return matched_gt_cells, matched_gt_classes, unmatched_gt_cells, unmatched_gt_classes, unmatched_pred_cells, unmatched_pred_classes\n",
    "    \n",
    "def cell_detection_postprocessing(y_tc, y_bc, y_bg, min_distance=3):\n",
    "    if isinstance(y_tc, torch.Tensor):\n",
    "        y_tc = y_tc.cpu().detach().numpy()\n",
    "    if isinstance(y_bc, torch.Tensor):\n",
    "        y_bc = y_bc.cpu().detach().numpy()\n",
    "    if isinstance(y_bg, torch.Tensor):\n",
    "        y_bg = y_bg.cpu().detach().numpy()\n",
    "    # Compute foreground probability\n",
    "    foreground = 1 - y_bg\n",
    "    foreground = cv2.GaussianBlur(foreground, (0, 0), sigmaX=3)\n",
    "    # Detect peaks (local maxima)\n",
    "    cell_candidates = peak_local_max(foreground, min_distance=min_distance, exclude_border=0, threshold_abs=0.0)\n",
    "\n",
    "    # Store valid cells, classes, and confidence scores\n",
    "    valid_cells = []\n",
    "    valid_classes = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    for x, y in cell_candidates:\n",
    "        if y_tc[x, y] > y_bg[x, y] or y_bc[x, y] > y_bg[x, y]:  \n",
    "            valid_cells.append((x, y))\n",
    "            \n",
    "            # Determine the predicted class (1 = background cell, 2 = tumor cell)\n",
    "            if y_bc[x, y] > y_tc[x, y]:\n",
    "                cell_class = 1  # Background cell\n",
    "            else:\n",
    "                cell_class = 2  # Tumor cell\n",
    "            \n",
    "            valid_classes.append(cell_class)\n",
    "\n",
    "            # Confidence score is the max probability of either class\n",
    "            confidence_score = max(y_tc[x, y], y_bc[x, y])\n",
    "            confidence_scores.append(confidence_score)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    valid_cells = np.array(valid_cells)\n",
    "    valid_classes = np.array(valid_classes)\n",
    "    confidence_scores = np.array(confidence_scores)\n",
    "\n",
    "    # Sort by confidence score (descending order)\n",
    "    sorted_indices = np.argsort(-confidence_scores)\n",
    "    return valid_cells[sorted_indices], valid_classes[sorted_indices], confidence_scores[sorted_indices]\n",
    "\n",
    "def cell_detection_postprocessing_batch(y_c_batch, min_distance=3):\n",
    "\n",
    "    cells_list = []\n",
    "    classes_list = []\n",
    "    confidences_list = []\n",
    "\n",
    "    for y_c in y_c_batch:\n",
    "        # Extract individual channels\n",
    "        y_bg, y_bc, y_tc = y_c[0], y_c[1], y_c[2]\n",
    "\n",
    "        # Process each batch item individually\n",
    "        cells, classes, confidences = cell_detection_postprocessing(y_tc, y_bc, y_bg, min_distance=min_distance)\n",
    "        cells_list.append(cells)\n",
    "        classes_list.append(classes)\n",
    "        confidences_list.append(confidences)\n",
    "\n",
    "    return cells_list, classes_list, confidences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 126/126 [01:54<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "best_f1 = -1\n",
    "worst_f1 = 2\n",
    "\n",
    "best_sample = None\n",
    "worst_sample = None\n",
    "all_samples = []\n",
    "\n",
    "for i, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "    gt_cells_list, gt_classes_list = load_ground_truth(batch)\n",
    "    meta_batch = batch[\"meta\"]\n",
    "    cropped_coords = normalize_crop_coords_batch(meta_batch)\n",
    "    \n",
    "    _, cell_pred = cell_classifier.net.forward(batch, cropped_coords, train_mode = False)\n",
    "    cell_pred = cell_post_transform(cell_pred)\n",
    "\n",
    "    # Get predicted cells and their classes from the model outputs\n",
    "    pred_cells_list, pred_classes_list, confidences_list = cell_detection_postprocessing_batch(cell_pred)\n",
    "\n",
    "    matched_gt_cells, matched_gt_classes, unmatched_gt_cells, unmatched_gt_classes, unmatched_pred_cells, unmatched_pred_classes = evaluate_cell_detection_with_coords(\n",
    "            pred_cells_list, pred_classes_list, gt_cells_list, gt_classes_list, max_distance=15)\n",
    "\n",
    "    total_tp_bc, total_fp_bc, total_fn_bc, total_tp_tc, total_fp_tc, total_fn_tc = evaluate_cell_detection_batch(\n",
    "                pred_cells_list, pred_classes_list, gt_cells_list, gt_classes_list,\n",
    "                )\n",
    "    \n",
    "    precision_bc, recall_bc, f1_bc = calculate_metrics(total_tp_bc, total_fp_bc, total_fn_bc)\n",
    "    precision_tc, recall_tc, f1_tc = calculate_metrics(total_tp_tc, total_fp_tc, total_fn_tc)\n",
    "\n",
    "    mean_f1 = (f1_bc + f1_tc) / 2\n",
    "    #print(f\"BC: {f1_bc}\")\n",
    "    #print(f\"TC: {f1_tc}\")\n",
    "\n",
    "\n",
    "    unique_classes = set([c for sublist in gt_classes_list for c in sublist])\n",
    "    if 1 in unique_classes and 2 in unique_classes:\n",
    "        sample = {\n",
    "            \"index\": i,\n",
    "            \"image\": batch[\"img_cell\"],\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"gt_classes_list\": gt_classes_list,\n",
    "            \"pred_classes_list\": pred_classes_list,\n",
    "            \"gt_cells_list\": gt_cells_list,\n",
    "            \"pred_cells_list\": pred_cells_list,\n",
    "            \"matched_gt_list\": [matched_gt_cells],\n",
    "            \"matched_gt_classes_list\": [matched_gt_classes],\n",
    "            \"unmatched_gt_list\": [unmatched_gt_cells],\n",
    "            \"unmatched_gt_classes_list\": [unmatched_gt_classes],\n",
    "            \"wrong_pred_cells_list\": [unmatched_pred_cells],\n",
    "            \"wrong_pred_classes_list\": [unmatched_pred_classes],\n",
    "        }\n",
    "\n",
    "        all_samples.append(sample)\n",
    "\n",
    "        if mean_f1 > best_f1:\n",
    "            best_f1 = mean_f1\n",
    "            best_sample = sample\n",
    "\n",
    "        if mean_f1 < worst_f1:\n",
    "            worst_f1 = mean_f1\n",
    "            worst_sample = sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_samples = random.sample(all_samples, 3) if len(all_samples) >= 3 else all_samples\n",
    "random_indices = [81, 14, 3]\n",
    "random_samples = [all_samples[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_cell_visualization(sample, save_path, title=\"\"):\n",
    "    image_tensor = sample[\"image\"]  # shape: [B, C, H, W]\n",
    "    index = sample[\"index\"]\n",
    "\n",
    "    # Visualize only the first image in the batch\n",
    "    image = image_tensor[0].detach().cpu() \n",
    "    image_np = image.permute(1, 2, 0).numpy()  # [H, W, C]\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())  # normalize to [0,1]\n",
    "\n",
    "    # Setup plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Mean F1: {sample['mean_f1']:.2f}\")\n",
    "\n",
    "    for (y, x), cls in zip(sample[\"matched_gt_list\"][0], sample[\"matched_gt_classes_list\"][0]):\n",
    "        if cls == 1:\n",
    "            plt.plot(x, y, 'o', color='g', markersize=3)\n",
    "        if cls == 2:\n",
    "            plt.plot(x, y, 'o', color='r', markersize=3)\n",
    "    # Plot FNs (missed GTs)\n",
    "    for (y, x), cls in zip(sample[\"unmatched_gt_list\"][0], sample[\"unmatched_gt_classes_list\"][0]):\n",
    "        color = 'b' if cls == 1 else 'orange'\n",
    "        plt.plot(x, y, 'o', color=color, markersize=3)\n",
    "\n",
    "    # Plot FPs (wrong predictions)\n",
    "    for (y, x), cls in zip(sample[\"wrong_pred_cells_list\"][0], sample[\"wrong_pred_classes_list\"][0]):\n",
    "        color = 'g' if cls == 1 else 'r'\n",
    "        marker = 'x'\n",
    "        plt.plot(x, y, marker, color=color, markersize=3)\n",
    "\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=1024, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cell_visualization(best_sample, f\"ocelot_cell/best/{name}_sample_best.png\", title=\"Best Sample\")\n",
    "save_cell_visualization(worst_sample, f\"ocelot_cell/worst/{name}_sample_worst.png\", title=\"Worst Sample\")\n",
    "i = 0\n",
    "for sample in random_samples:\n",
    "    save_cell_visualization(sample, f\"ocelot_cell/random/{name}_random_sample_{i}.png\", title=f\"Random Sample {i}\")\n",
    "    i +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
